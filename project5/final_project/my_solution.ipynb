{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schoolboy/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bonus': 64,\n",
       " 'deferral_payments': 107,\n",
       " 'deferred_income': 97,\n",
       " 'director_fees': 129,\n",
       " 'email_address': 35,\n",
       " 'exercised_stock_options': 44,\n",
       " 'expenses': 51,\n",
       " 'from_messages': 60,\n",
       " 'from_poi_to_this_person': 60,\n",
       " 'from_this_person_to_poi': 60,\n",
       " 'loan_advances': 142,\n",
       " 'long_term_incentive': 80,\n",
       " 'other': 53,\n",
       " 'poi': 0,\n",
       " 'restricted_stock': 36,\n",
       " 'restricted_stock_deferred': 128,\n",
       " 'salary': 51,\n",
       " 'shared_receipt_with_poi': 60,\n",
       " 'to_messages': 60,\n",
       " 'total_payments': 21,\n",
       " 'total_stock_value': 20}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "%matplotlib inline\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary']\n",
    "# You will need to use more features\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "# missing value count \n",
    "total_feature = my_dataset['ALLEN PHILLIP K'].keys()\n",
    "feature_count_dict = {}\n",
    "for val in range(0,len(total_feature)):\n",
    "    feature_count_dict[total_feature[val]] = 0\n",
    "poi_count = 0\n",
    "for val in my_dataset:\n",
    "    for key in my_dataset[val]:\n",
    "        if key == 'poi':\n",
    "            if my_dataset[val][key] == True:\n",
    "                poi_count+=1\n",
    "        if my_dataset[val][key] == 'NaN':\n",
    "            feature_count_dict[key]+=1\n",
    "# total length of data\n",
    "print len(my_dataset)\n",
    "# true poi count\n",
    "print poi_count\n",
    "# feature nan count\n",
    "\n",
    "feature_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing feature which has NaN value more than 100\n",
    "total_feature.remove('deferred_income')\n",
    "total_feature.remove('director_fees')\n",
    "total_feature.remove('loan_advances')\n",
    "total_feature.remove('restricted_stock_deferred')\n",
    "total_feature.remove('email_address')\n",
    "total_feature.remove('poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del my_dataset['TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# new feature formation \n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "for name in my_dataset:\n",
    "    if (all([\n",
    "        my_dataset[name]['from_messages'] != 'NaN',\n",
    "        my_dataset[name]['from_this_person_to_poi'] != 'NaN',\n",
    "        my_dataset[name]['to_messages'] != 'NaN',\n",
    "        my_dataset[name]['from_poi_to_this_person'] != 'NaN'\n",
    "    ])):\n",
    "        my_dataset[name]['from_fraction'] = float(my_dataset[name]['from_poi_to_this_person']) / float(my_dataset[name]['to_messages'])\n",
    "        my_dataset[name]['to_fraction'] = float(my_dataset[name]['from_this_person_to_poi']) / float(my_dataset[name]['from_messages'])\n",
    "    else:\n",
    "        my_dataset[name]['from_fraction'] = 0\n",
    "        my_dataset[name]['to_fraction'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_feature.insert(0,'from_fraction')\n",
    "total_feature.insert(0,'to_fraction')\n",
    "my_features = total_feature\n",
    "data = featureFormat(my_dataset, my_features, sort_keys = True)\n",
    "\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k should be >=0, <= n_features; got 7.Use k='all' to return all features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-39fb5350a241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mk_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mk_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresults_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schoolboy/miniconda2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    327\u001b[0m                             % (self.score_func, type(self.score_func)))\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schoolboy/miniconda2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    472\u001b[0m             raise ValueError(\"k should be >=0, <= n_features; got %r.\"\n\u001b[1;32m    473\u001b[0m                              \u001b[0;34m\"Use k='all' to return all features.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                              % self.k)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: k should be >=0, <= n_features; got 7.Use k='all' to return all features."
     ]
    }
   ],
   "source": [
    "my_features = total_feature\n",
    "k_best = SelectKBest(k=7)\n",
    "k_best.fit(features, labels)\n",
    "results_list = zip(k_best.get_support(), my_features[1:],k_best.scores_)\n",
    "best_features = []\n",
    "for i in range(0,len(results_list)):\n",
    "    if results_list[i][0] == True:\n",
    "        best_features.insert(0,results_list[i][2])\n",
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator\n",
      "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.63      0.65        19\n",
      "        1.0       0.00      0.00      0.00         3\n",
      "        2.0       0.00      0.00      0.00         1\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "        5.0       0.00      0.00      0.00         1\n",
      "        6.0       0.00      0.00      0.00         1\n",
      "        7.0       0.00      0.00      0.00         1\n",
      "        8.0       0.00      0.00      0.00         1\n",
      "       14.0       0.00      0.00      0.00         1\n",
      "       15.0       0.00      0.00      0.00         1\n",
      "       16.0       0.00      0.00      0.00         0\n",
      "       19.0       0.00      0.00      0.00         1\n",
      "       24.0       0.00      0.00      0.00         1\n",
      "       30.0       0.00      0.00      0.00         1\n",
      "       37.0       0.00      0.00      0.00         1\n",
      "       38.0       0.00      0.00      0.00         1\n",
      "       48.0       0.00      0.00      0.00         1\n",
      "       49.0       0.00      0.00      0.00         1\n",
      "       61.0       0.00      0.00      0.00         1\n",
      "       65.0       0.00      0.00      0.00         1\n",
      "      108.0       0.00      0.00      0.00         1\n",
      "      411.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.31      0.29      0.30        41\n",
      "\n",
      "best estimator\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=5,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      1.00      0.91        15\n",
      "        1.0       0.00      0.00      0.00         3\n",
      "        2.0       0.00      0.00      0.00         1\n",
      "        4.0       0.00      0.00      0.00         1\n",
      "        5.0       0.00      0.00      0.00         0\n",
      "        6.0       0.00      0.00      0.00         2\n",
      "        7.0       0.00      0.00      0.00         1\n",
      "        8.0       0.00      0.00      0.00         1\n",
      "       11.0       0.00      0.00      0.00         1\n",
      "       12.0       0.00      0.00      0.00         1\n",
      "       13.0       0.00      0.00      0.00         0\n",
      "       14.0       0.00      0.00      0.00         1\n",
      "       15.0       0.00      0.00      0.00         3\n",
      "       16.0       0.00      0.00      0.00         1\n",
      "       20.0       0.00      0.00      0.00         1\n",
      "       21.0       0.00      0.00      0.00         1\n",
      "       26.0       0.00      0.00      0.00         1\n",
      "       30.0       0.00      0.00      0.00         1\n",
      "       48.0       0.00      0.00      0.00         1\n",
      "       49.0       0.00      0.00      0.00         1\n",
      "       61.0       0.00      0.00      0.00         1\n",
      "       65.0       0.00      0.00      0.00         0\n",
      "      108.0       0.00      0.00      0.00         1\n",
      "      194.0       0.00      0.00      0.00         0\n",
      "      386.0       0.00      0.00      0.00         0\n",
      "      387.0       0.00      0.00      0.00         1\n",
      "      609.0       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.30      0.37      0.33        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "data = featureFormat(my_dataset, best_features, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "def evaluate_clf(clf,params,feature,lable):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature, lable,train_size=.7)\n",
    "    clf= GridSearchCV(clf, params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print \"best estimator\"\n",
    "    print clf.best_estimator_\n",
    "    y_pred = clf.predict(X_test)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    print classification_report(y_test,y_pred)\n",
    "'''    for i in range(0,len(y_test)):\n",
    "        #precision.append(precision_score(y_test[i],y_pred[i]))\n",
    "        #recall.append(recall_score(y_test[i],y_pred[i]))\n",
    "        print precision_score(y_test[i],y_pred[i])\n",
    "    print mean(recall)\n",
    "    print mean(precision)\n",
    " '''\n",
    "best_features.insert(0,'poi')\n",
    "#clf.fit()\n",
    "lsvc=svm.LinearSVC(penalty='l2')\n",
    "# C parameter is used for how much classification curve has to make\n",
    "evaluate_clf(lsvc,[{'C':[1,10,100]}],features,labels)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rforest = RandomForestClassifier(n_estimators = 10)\n",
    "# min samples leaf is used to determine how much sample minimum requrired to go further split\n",
    "evaluate_clf(rforest,[{'min_samples_leaf':[5,10,20]}],features,labels)\n",
    "rforest = RandomForestClassifier(n_estimators = 10,min_samples_leaf = 5)\n",
    "dump_classifier_and_data(rforest, my_dataset, best_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
